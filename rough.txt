from pathlib import Path
import pandas as pd
from difflib import SequenceMatcher
import logging
import os  # Add missing import
from typing import List, Dict, Optional, Tuple
import openai
from .logger import setup_logging

# Setup logger for customer name matching
logger = setup_logging('customer_name_matching')

def similarity(a, b):
    """Calculate similarity ratio between two strings."""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def normalize_customer_name(name):
    """Normalize customer name for better matching."""
    if pd.isna(name) or name == "":
        return ""
    
    name = str(name).strip()
    # Handle URL encoding
    name = name.replace('%26', '&')
    name = name.replace('%20', ' ')
    # Remove extra spaces
    name = ' '.join(name.split())
    return name

def find_best_match_in_dataframe(input_name, df, name_columns, similarity_threshold=0.7):
    """Find best match in a dataframe across multiple name columns."""
    best_match = None
    best_similarity = 0
    best_match_type = None
    
    for _, row in df.iterrows():
        for col_name in name_columns:
            if col_name not in df.columns:
                continue
                
            candidate_name = normalize_customer_name(row[col_name])
            if not candidate_name:
                continue
                
            sim_ratio = similarity(input_name, candidate_name)
            
            if sim_ratio > best_similarity and sim_ratio >= similarity_threshold:
                best_similarity = sim_ratio
                best_match = row
                best_match_type = col_name
    
    return best_match, best_similarity, best_match_type

def update_customer_name_dual_matching(customer_db_path, bc_cache_path, input_file, 
                                     local_threshold=0.95, bc_threshold=0.75):
    """Enhanced customer matching with dual-stage process and separate thresholds."""
    logger.info(f"Starting dual-stage customer name update for {input_file}")
    logger.info(f"Local customer database: {customer_db_path}")
    logger.info(f"Business Central cache: {bc_cache_path}")
    logger.info(f"Local DB threshold: {local_threshold}")
    logger.info(f"Business Central threshold: {bc_threshold}")
    
    try:
        # Load all required files
        customer_df = pd.read_csv(customer_db_path)
        input_df = pd.read_csv(input_file)
        
        # Load Business Central cache if it exists
        bc_df = None
        if os.path.exists(bc_cache_path):
            bc_df = pd.read_csv(bc_cache_path)
            logger.info(f"Loaded {len(bc_df)} Business Central cached customers")
        else:
            logger.warning(f"Business Central cache not found: {bc_cache_path}")
        
        logger.info(f"Loaded {len(customer_df)} local database entries")
        logger.info(f"Processing {len(input_df)} input rows")
        
    except Exception as e:
        logger.error(f"Failed to load files: {e}")
        return
    
    # Check required columns
    local_db_columns = ["SPECIAL NAME BANK IN", "CUSTOMER NAME"]
    bc_cache_columns = ["CUSTOMER_NAME", "CONTACT"]
    
    if "CUSTOMER_NAME" not in input_df.columns:
        logger.error("'CUSTOMER_NAME' column not found in input file")
        return
    
    updated_count = 0
    processed_count = 0
    skipped_count = 0
    local_matches = 0
    bc_matches = 0
    
    for index, row in input_df.iterrows():
        input_customer_name = normalize_customer_name(row["CUSTOMER_NAME"])
        
        # Skip if customer name is empty
        if not input_customer_name:
            logger.debug(f"Row {index+1}: Skipping empty customer name")
            skipped_count += 1
            continue

        processed_count += 1
        logger.debug(f"Row {index+1}: Processing '{input_customer_name}'")
        
        # Stage 1: Try to match against local customer database with local_threshold
        best_match, best_similarity, match_type = find_best_match_in_dataframe(
            input_customer_name, customer_df, local_db_columns, local_threshold
        )
        
        if best_match is not None:
            # Found match in local database
            new_customer_name = normalize_customer_name(best_match["CUSTOMER NAME"])
            input_df.at[index, "CUSTOMER_NAME"] = new_customer_name
            updated_count += 1
            local_matches += 1
            
            logger.info(f"Row {index+1}: LOCAL MATCH via {match_type} - '{row['CUSTOMER_NAME']}' -> '{new_customer_name}' (similarity: {best_similarity:.3f})")
            continue
        
        # Stage 2: Try to match against Business Central cache with bc_threshold
        if bc_df is not None:
            bc_match, bc_similarity, bc_match_type = find_best_match_in_dataframe(
                input_customer_name, bc_df, bc_cache_columns, bc_threshold
            )
            
            if bc_match is not None:
                # Found match in Business Central cache
                new_customer_name = normalize_customer_name(bc_match["CUSTOMER_NAME"])
                input_df.at[index, "CUSTOMER_NAME"] = new_customer_name
                updated_count += 1
                bc_matches += 1
                
                logger.info(f"Row {index+1}: BC MATCH via {bc_match_type} - '{row['CUSTOMER_NAME']}' -> '{new_customer_name}' (similarity: {bc_similarity:.3f})")
                continue
        
        # No match found in either database
        logger.warning(f"Row {index+1}: NO MATCH - '{input_customer_name}'")
    
    # Save the updated dataframe
    input_df.to_csv(input_file, index=False)
    
    # Log final statistics
    logger.info(f"Dual-stage customer name matching completed:")
    logger.info(f"  Total rows: {len(input_df)}")
    logger.info(f"  Processed: {processed_count}")
    logger.info(f"  Skipped (empty): {skipped_count}")
    logger.info(f"  Local DB matches (≥{local_threshold}): {local_matches}")
    logger.info(f"  Business Central matches (≥{bc_threshold}): {bc_matches}")
    logger.info(f"  Total matched: {updated_count}")
    logger.info(f"  No matches found: {processed_count - updated_count}")
    logger.info(f"  Overall match rate: {(updated_count/processed_count*100):.1f}%" if processed_count > 0 else "  Match rate: 0%")
    
    print(f"Updated {updated_count} customer names in {input_file}")
    print(f"Local DB matches: {local_matches}, Business Central matches: {bc_matches}")

def update_customer_name_for_file_dual(processed_file_path, local_threshold=0.95, bc_threshold=0.75):
    """Auto-determine databases and perform dual-stage matching with separate thresholds."""
    processed_file_path = Path(processed_file_path)
    
    if not processed_file_path.exists():
        logger.error(f"Processed file not found: {processed_file_path}")
        return False
    
    # Determine database paths based on file name
    file_name = processed_file_path.name.upper()
    
    database_mapping = {
        "MBB_2025_PROCESSED.CSV": {
            "local_db": "data/customer_db/MY_MBB_CUSTOMER_NAME.csv",
            "bc_cache": "data/customer_db/BC_MY_CUSTOMERS.csv"
        },
        "PBB_2025_PROCESSED.CSV": {
            "local_db": "data/customer_db/MY_PBB_CUSTOMER_NAME.csv", 
            "bc_cache": "data/customer_db/BC_MY_CUSTOMERS.csv"
        },
        "SG_MBB_2025_PROCESSED.CSV": {
            "local_db": "data/customer_db/SG_MBB_customer_name.csv",
            "bc_cache": "data/customer_db/BC_MY_CUSTOMERS.csv"
        },
        "SMARTHOME_MBB_2025_PROCESSED.CSV": {
            "local_db": "data/customer_db/MY_MBB_CUSTOMER_NAME.csv",
            "bc_cache": "data/customer_db/BC_MY_CUSTOMERS.csv"
        }
    }
    
    config = None
    for pattern, db_config in database_mapping.items():
        if pattern in file_name:
            config = db_config
            break
    
    if config is None:
        logger.error(f"No database mapping found for file: {file_name}")
        return False
    
    local_db_path = Path(config["local_db"])
    bc_cache_path = Path(config["bc_cache"])
    
    if not local_db_path.exists():
        logger.error(f"Local customer database not found: {local_db_path}")
        return False
    
    logger.info(f"Starting dual-stage matching for {processed_file_path}")
    logger.info(f"Local DB: {local_db_path} (threshold: {local_threshold})")
    logger.info(f"BC Cache: {bc_cache_path} (threshold: {bc_threshold})")
    
    try:
        update_customer_name_dual_matching(
            customer_db_path=str(local_db_path),
            bc_cache_path=str(bc_cache_path),
            input_file=str(processed_file_path),
            local_threshold=local_threshold,
            bc_threshold=bc_threshold
        )
        return True
    except Exception as e:
        logger.error(f"Error during dual-stage matching: {e}")
        return False

# Add wrapper function for backward compatibility
def update_customer_name_for_file(processed_file_path, local_threshold=0.95, bc_threshold=0.75):
    """Wrapper function for backward compatibility with existing workflow."""
    return update_customer_name_for_file_dual(processed_file_path, local_threshold, bc_threshold)